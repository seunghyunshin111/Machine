# Regression_code 설명



## **단순 선형회귀 모델 구현하기**

이론 강의를 통해 입력값 X*X*가 1개인 경우 사용하는 알고리즘인 단순 선형회귀 모델에 대해 알아보았습니다.

실습 1에서와 같이 회귀 모델의 함수식 Y = \beta_0 + \beta_1 X*Y*=*β*0+*β*1*X*을 이용하여 알고리즘을 직접 구현할 수도 있지만, 사이킷런이라고 하는 라이브러리에 구현되어 있는 모델을 불러오는 것만으로도 쉽게 단순 선형회귀 알고리즘을 사용할 수 있습니다.

이번 시간에는 사이킷런을 활용하여 주어진 데이터에 대한 단순 선형회귀를 구현하는 방법을 익혀보도록 합시다.

실습을 수행하는 데 필요한 사이킷런 함수와 라이브러리는 아래 내용을 참고하실 수 있습니다.



------



**데이터 준비를 위한 사이킷런 함수/라이브러리**

- `from sklearn.model_selection import train_test_split `: 학습용 데이터와 테스트용 데이터를 나누어주는 기능을 불러옵니다.
- `train_test_split(X, y, test_size=0.3, random_state=0)`: 데이터의 70%를 학습에 사용하고, 나머지 30%의 데이터를 테스트용 데이터로 나눈 결과를 반환합니다.



**단순 선형회귀와 관련된 사이킷런 함수/라이브러리**

- `from sklearn.linear_model import LinearRegression`: 단순 선형회귀 모델(클래스)을 불러옵니다.
- `simplelinear = LinearRegression()` : 모델을 정의합니다.
- `simplelinear.fit(X, y)`: (X, y) 데이터셋에 대해서 모델을 학습시킵니다.
- `simplelinear.predict(X)`: X 데이터를 바탕으로 예측되는 값을 출력합니다.
- `simplelinear.score(X,y)` : 학습이 완료된 모델의 평가 점수를 출력합니다.
- `simplelinear.coef_` : 학습한 모델의 \beta_1*β*1 를 반환합니다.
- `simplelinear.intercept_` : 학습한 모델의 \beta_0*β*0 를 반환합니다.
- 

------



## **실습**



1. 랜덤 생성된 입력값 X와 y 데이터를 아래와 같이 학습용, 테스트용 데이터로 분리합니다.
   - 학습용 데이터 : 전체 데이터의 `70%`
   - 테스트용 데이터 : 전체 데이터의 `30%`
   - random_state :`0`

1. 사이킷런에 구현되어 있는 단순 선형회귀 모델을 불러와 `simplelinear` 변수에 저장합니다.
2. 불러온 회귀 모델을 학습용 데이터(`train_X`, `train_y`)에 맞추어 학습시킵니다.
3. 학습이 완료되었으면, 테스트용 데이터(`test_X`)의 회귀 결과를 예측하고, 예측 결과를 `predicted` 변수에 저장합니다.
4. 사이킷런 회귀 모델 내에 구현되어 있는 score 함수를 사용하여 모델 학습 평가 점수를 `model_score` 변수에 저장합니다.
   - test_X와 test_y에 대한 모델 평가 점수를 저장해야 합니다.
5. 회귀 모델을 통해 찾은 최적의 \beta_0*β*0와 \beta_1*β*1 를 각각 변수 `beta_0`와 `beta_1` 에 저장하고, 출력을 통해 확인해봅니다.
   - 단순 선형회귀 식 $Y = beta_0 + beta_1 X$와 출력된 두 변수값을 통해 X와 y 간의 관계를 생각해보세요!
6. 실행 버튼을 눌러 모델이 학습되고 난 이후의 그래프 결과와 모델 점수를 통해 회귀 선을 확인해보세요.
   - 아직 회귀 알고리즘의 평가 지표에 대해 학습하지 않았지만, 모델 점수는 높을 수록 예측이 잘 된 모델이라고 할 수 있습니다.
7. 제출 버튼을 눌러 단순 선형회귀 모델이 잘 구현되었는지 확인해보세요.



<br>



## **다중 선형회귀 모델 구현하기**



다중 선형회귀는 입력값이 1개일 경우 적용하는 단순 선형회귀 알고리즘과 달리 입력값 X*X*가 여러 개일 때 사용할 수 있는 회귀 알고리즘입니다.

다중 선형회귀 또한 사이킷런에 구현되어 있는 라이브러리를 활용하여 간단하게 모델을 구현해보겠습니다.

단순 선형회귀 실습을 통해 `LinearRegression` 클래스를 정의하고 사용한 것을 기억하시나요?

사실 위 클래스는 다중 선형회귀에서도 사용이 가능합니다. 사이킷런에서는 선형회귀 라는 이름으로 단순/다중 선형회귀의 구분 없이 동일한 모듈을 활용할 수 있습니다.

sklearn 에 저장된 데이터를 불러오고, 불러온 데이터를 다중 선형회귀 모델을 사용해 예측을 진행해보도록 하겠습니다.



------

**sklearn 에 저장된 데이터 불러오기**
사이킷런에는 간단하고 다양한 데이터들이 저장되어 있어, 불러오기 만으로 간단하게 사용할 수 있습니다.

- `from sklearn.datasets import load_boston` : 사이킷런의 boston 데이터를 불러옵니다.
- `load_boston(return_X_y = True)` : (X, y) 형태의 boston 데이터를 반환합니다.

**다중 선형회귀와 관련된 사이킷런 함수/라이브러리**

- `from sklearn.linear_model import LinearRegression`: 다중 선형회귀 모델(클래스)을 불러옵니다.
- `multilinear = LinearRegression()`: 다중 선형 회귀 모델을 정의합니다.
- `multilinear.fit(x, y)`: (x, y) 데이터셋에 대해서 모델을 학습시킵니다.
- `multilinear.predict(x)`: x 데이터를 바탕으로 예측되는 값을 출력합니다.
- `multilinear.score(x,y)` : 학습한 모델의 평가 점수를 출력합니다.
- `multilinear.coef_` : 학습한 모델의 \beta_i*β**i* 를 반환합니다.
- `multilinear.intercept_` : 학습한 모델의 \beta_0*β*0 를 반환합니다.

------





## **실습**

1. sklearn 에 저장되어 있는 boston 데이터를 (X, y) 형태로 불러와 각각 X와 y에 저장합니다.
2. 불러온 데이터를 학습용 데이터와 테스트용 데이터로 분리합니다.
   - 학습용 데이터 : 전체 데이터의 `80%`
   - 테스트 데이터 : 전체 데이터의 `20%`
   - random_state : `100`
3. 사이킷런에 구현된 다중 선형회귀 모델을 불러와 변수 `multilinear`에 저장합니다.
   - 사이킷런에서는 선형회귀라는 이름으로 단순/다중 선형회귀의 구분 없이 동일한 모듈을 활용할 수 있습니다.
4. 불러온 모델을 학습용 데이터를 기반으로 학습시킵니다.
5. 학습이 완료되었으면, 테스트 데이터에 대한 예측을 수행하고, 예측 결과값을 변수 `predicted` 에 저장합니다.
6. `score()` 함수를 사용하여 모델 학습 평가 점수를 확인해봅니다. 단순 선형회귀에서와 마찬가지로 결과 점수는 높을 수록 예측이 잘 된 모델이라고 할 수 있습니다.
7. 최적의 \beta_0*β*0와 각 입력값 X_i*X**i*들에 대한 \beta_i*β**i* 값을 변수 `beta_0` 와 `beta_i_list` 에 저장합니다.



<br>



## **다항 회귀 모델 구현하기**

다항 회귀는 Y를 X에 대한 임의의 다항 함수로 모델링 하는 선형 회귀를 의미합니다.

강의를 통해 알아본 바와 같이 다항 회귀는 다중 회귀와 동일한 원리를 가지고 있습니다.

다항 회귀는 입력 데이터 X에 대한 전처리를 진행해준 후 다중 선형회귀를 진행하기 때문에 사이킷런에서도 다항 회귀를 위해서는 입력 데이터를 변환 해준 후 변환한 데이터를 동일한 다중 선형회귀 모델에 적용합니다.

이번 시간에는 이러한 다항 함수를 사이킷런을 이용하여 구현해보겠습니다.

------

**유용한 함수**

- ```
  PolynomialFeatures(degree)
  ```

  : Polynomial 객체를 생성합니다.

  - degree: 만들어줄 다항식의 차수를 의미합니다.

- `PolynomialFeatures.fit_transform(x)`:x와 x의 degree제곱을 한 변환된 입력값을 반환합니다.





## **실습**

1. 데이터 내의 각 특성 값을 제곱하여 새로운 특성(feature)을 추가하는 PolynomialFeature 객체를 생성합니다.
   - degree : 2
   - include_bias : False
   - include_bias 파라미터를 True로 설정하게 되면, 해당 다항식의 모든 거듭제곱이 0일 경우 편향 변수를 추가합니다. 이는 회귀식에서 $\beta_0$ 와 같은 역할을 합니다.
2. 각 데이터를 제곱하고 이를 특성에 추가시킨 후 `poly_x`에 저장합니다.
   - sklearn의 PolynomialFeature객체를 이용하면 간단하게 각 데이터를 제곱하고 기존 X에 추가시킨 데이터를 반환할 수 있습니다.
   - 

```
예: x=[0, 1, 2, 3] >> poly_x=[[0,0], [1,1], [2,4], [3, 9]])Copy
```

1. 다중 선형회귀 모델을 생성하여 `linear_model`에 저장합니다.
2. 생성한 다중 선형회귀 모델로 변환된 전체 데이터와 y를 모델에 학습시킵니다.
3. 학습된 모델의 예측 결과값을 `predicted` 에 저장합니다.